{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RefaelAharouni/Project_Machine_Learning_Refael_Venkatsai_Norbert_William/blob/Refa%C3%ABl/Project_Machine_Learning_Refael_Venkatsai_Norbert_William.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8VYnZOCiyov"
      },
      "source": [
        "Aharouni Refaël, Kadari Venkatsai, Devaraj Norbert Dias, Aye William $-$ DIA1\n",
        "<br><br>\n",
        "\n",
        "<p align=\"center\"><b> Report of the Machine Learning Project</b></p><br><br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDyZ74l32ZFb"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;In this dataset, we want to study the relationship between the presence of diabetes for patients and features like the patient's age, his geographical location, his potential heart diseases and level of blood glucoses for example. Therefore, this problem is a binary classification one (we want to know if the patient has a diabete (the target feature) or not, given his personal information)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOqYHcfctk1z"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;Let us first import the libraries that will be used for this project. We can then read the the dataset with the *pd.read_csv* method and store it in the *data* variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71w-9PYtgD6V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.metrics import (confusion_matrix, roc_curve, precision_recall_curve, auc, classification_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wesz0BXGzHxW"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"diabetes_dataset.csv\", sep = \",\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj05hnLCu-H_"
      },
      "source": [
        "Now, we can print the first lines of the dataset to see what it looks like using *head* method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk_mpwfwz9px"
      },
      "outputs": [],
      "source": [
        "data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdP40S-j5bzp"
      },
      "source": [
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPevbz69hZ_R"
      },
      "source": [
        "# **1. Analysis and preprocessing of the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iStWfen0wWcw"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;Let us begin this project by analyzing the data.\n",
        "<br>\n",
        "## &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**A. Variables definition**\n",
        "&nbsp;&nbsp;&nbsp;The first thing to do is understanding the variables that we are dealing with in this dataset using the *columns.tolist()* method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPsduehYDbBd"
      },
      "outputs": [],
      "source": [
        "print(data.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEJJ0cBW54-g"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;From the previous result, we can draw up a list of the features and their type:\n",
        "\n",
        "*   **Year**: This feature represents the year during which the test has been done on the patient. At first glance, we could think that this variable is *quantitative continuous* because in theory, it can take an infinite number of values. However, after using the command: \"len(data[\"year\"].value_counts())\", we saw that there were only 7 different values taken for this feature, which is largely smaller than 100 000, the number of individuals. Therefore, this variable can be considered as *quantitative discrete*. <br><br>\n",
        "\n",
        "*   **Gender:** This feature represents the gender of the patient, so it is a *qualitative nominale* variable (the possible values are: \"male\", \"female\" or \"other\"). <br><br>\n",
        "\n",
        "*   **Age:** This feature is *quantitative discrete*, just as the \"year\" one (even if, once again, it can take every value between 0 and 150). It represents the age of the patient when the test to know if he had the diabete was made. <br><br>\n",
        "\n",
        "*   **Location:** This variable represents the location of the patient (the American state of the patient). Therefore, this variable is *qualitative nominale*.<br><br>\n",
        "\n",
        "*   **race:AfricanAmerican**, **race:Asian**, **race:Caucasian**, **race:Hispanic**, **race:Other**: Those variables indicate if the patient is African/American or not, Asian or not, Caucasian or not, Hispanic or not, or other (or not). Therefore, they are *quantitative binary*. <br><br>\n",
        "\n",
        "*  **hypertension**: Quantitative binary as it indicates if the patient is in hypertension or not (1 --> Hypertension, 0 --> no hypertension)<br><br>\n",
        "\n",
        "*  **heart_disease:** Quantitative binary as it indicates if the patient has a heart disease or not (1 --> heart_disease, 0 --> no heart_disease). <br><br>\n",
        "\n",
        "*  **smoking_history:** Qualitative ordinal because it indicates the patient's smoking frequency (with a hierarchy). The possible values are: \"never\" (level 0), \"not current\" (level 1), \"former\" (level 2), \"ever\" (level 3), \"current\" (level 4).<br><br>\n",
        "\n",
        "*  **bmi:** This variable represents the ratio between the patient's weight and his height squared. Because it can take an infinite number of values, it is a *quantitative continuous* feature. <br><br>\n",
        "\n",
        "*  **hbA1c_level:** At first glance, it seemed that this feature, representing the hemoglobin percentage of sugar in the blood, was *quantitative continuous*. However, just like the *year* feature, after using the command: \"len(data[\"hbA1c_level\"].value_counts())\", we saw that there were only 18 different values taken for this feature, which is largely smaller than 100 000, the number of individuals. Therefore, this variable can also be considered as *quantitative discrete*. <br><br>\n",
        "\n",
        "*  **blood_glucose_level:** This feature characterizes the level of the patient's blood glucose (it is expressed in mg/dL). Because it also takes 18 distinct values, it will be considered as *quantitative discrete*. <br><br>\n",
        "\n",
        "*  **diabetes:** *Quantitative binary* as it indicates if the patient has a diabete or not (1 --> Diabete, 0 --> No diabete)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyVFdAYY2KEe"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qi833iN8Aeh"
      },
      "source": [
        "## &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**B. Number of columns, rows and values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9815fEdk8efX"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;In order to know the exact number of columns and rows, we can use the *.shape* method. The first argument will be the number of lines (number of individuals) while the second one will represent the number of columns (number of features)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b28tDSNn812E"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJZdyLM-9Zc0"
      },
      "source": [
        "Therefore, our dataset contains 100 000 individuals and 16 features. Using the *size* function allows us to know that our diabetes dataset contains 160 000 values, which is normal because we have 100 000 individuals and 16 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP1oriOy-iSJ"
      },
      "outputs": [],
      "source": [
        "data.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t0sXXuI_F7m"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bivK4oZY_Hqw"
      },
      "source": [
        "## &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**C. Missing values, inconsistencies and outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeFwMGnL_dlc"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;Let us now use *.info* and *.isnull.sum()* to see if our dataset contains any missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k22hPhwcgJSA"
      },
      "outputs": [],
      "source": [
        "print(data.info())\n",
        "print(\"\\nNumber of missing values for each feature:\")\n",
        "print(data.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0ATdB8MxZ0q"
      },
      "source": [
        "From what we can see, this dataset does not seem to contain any missing values.<br>However, before going any further, a little detail required our attention, especially for the \"age\" feature, which is in float64. To simplify the study, let us convert this feature's type into integer. It will prevent the dataset from having for example 0.54 for the age."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"age\"] = data[\"age\"].astype(int)"
      ],
      "metadata": {
        "id": "iCj5Eu2ajiTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>We also need to ensure that diabetes_dataset.csv does not have any inconsistencies. This step implies to look at the potential spelling mistakes, incoherent spaces or incoherent uppercases for the qualitative columns.\n",
        "\n",
        "Let us look at the values taken by each qualitative feature with the *data[name_feature].value_counts()* method, knowing from the *info* method precedently used that all qualitative features are \"object\" in our dataset."
      ],
      "metadata": {
        "id": "Lib1oTFhjggk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmnGpbwhEyif"
      },
      "outputs": [],
      "source": [
        "for j in data.select_dtypes(include = [\"object\"]).columns: # We select the qualitative columns and print the occurrencies of every value taken by each feature\n",
        "  print(data[j].value_counts(), \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORXV2s4mHdas"
      },
      "source": [
        "No spelling mistakes or additional spaces appear in our dataset. <br>However, even if it did not seem so at first glance, \"smoking_history\" contains more than 35 800 null values (that were registered as \"No Info\") and  will be treated once we will use one-hot encoder. For this feature, let us also use *map* function to rename the values taken and make them begin with uppercases. To make sure of the renaming, we can once again print the occurrencies of this feature's values using *.value_counts()* method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1K7NOzwI5mv"
      },
      "outputs": [],
      "source": [
        "data[\"smoking_history\"] = data[\"smoking_history\"].map({\"No Info\": \"No Info\", \"never\": \"Never\", \"former\": \"Former\", \"current\": \"Current\", \"not current\": \"Not Current\", \"ever\": \"Ever\"})\n",
        "data[\"smoking_history\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4v8y965KnKa"
      },
      "source": [
        "<br><br>\n",
        "&nbsp;&nbsp;&nbsp;Let us now evaluate the possible inconsistencies for quantitative features. For example, let us verify that:\n",
        "* Every patient's age is between 0 and 122 (the official human longevity record is held by Jeanne Louise Calment, who lived 122 years and 164 days, according to Statista).\n",
        "* Every patient's BMI is between 6.7 kg/m$^2$ and 98 kg/m$^2$ (the highest BMI ever registered was 98 kg/m$^2$ according to the article of PreciDIAB's website, while the lowest was 6.7 kg/m$^2$ according to Psychiatria Polska's article).\n",
        "* Every patient's blood glucose level is between 40 mg/dL and 600 mg/dL (according to Guideline Central's article, a blood glucose level greater than 600 mg/dL is very serious while American Diabetes Association tells us that a blood glucose level below 70 is characteristic of a diabetic patient).\n",
        "* Every patient's hbA1c is between 0% and 20%.\n",
        "* Every year in the dataset is between 1900 and 2025 as it represents the year where the diabete test has been done on the patients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97bvgUgmRqrE"
      },
      "outputs": [],
      "source": [
        "print(\"Inconsistent ages: \", data[(data[\"age\"] < 0) | (data[\"age\"] > 122)])\n",
        "print(\"Inconsistent years: \", data[(data[\"year\"] < 1900) | (data[\"year\"] > 2025)])\n",
        "print(\"Inconsistent BMI: \", data[(data[\"bmi\"] < 6.7) | (data[\"bmi\"] > 98)])\n",
        "print(\"Inconsistent blood glucose level: \", data[(data[\"blood_glucose_level\"] < 40) | (data[\"blood_glucose_level\"] > 600)])\n",
        "print(\"Inconsistent hbA1C level: \", data[(data[\"hbA1c_level\"] < 0) | (data[\"hbA1c_level\"] > 20)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wSZVd5BVOv4"
      },
      "source": [
        "Therefore, no inconstency seems to appear in our dataset.\n",
        "<br><br><br>&nbsp;&nbsp;&nbsp;To go further in our analysis, let us display the boxplot of every quantitative feature to see what the outliers in our dataset are (the outliers are the points that are out of the whiskers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JIw6h1UVkob"
      },
      "outputs": [],
      "source": [
        "for j in data.select_dtypes(include = [\"int64\", \"float64\"]).columns: # We select the quantitative features\n",
        "  plt.figure(figsize = (4, 4)) # Creation of a new figure\n",
        "  sns.boxplot(data[j]) # Display of boxplot\n",
        "  plt.title(f\"Boxplot of {j}\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w_AQABPimB6"
      },
      "source": [
        "From what one can see here:\n",
        "* The feature \"Year\" has some outliers, but this is because more than 75% of patients had their test in 2019 (hence, all the other values are considered as outliers). However, all values are between 2015 and 2022, so none of them is really \"extreme\". <br>\n",
        "* The feature \"age\" does not have any extreme values. <br>\n",
        "* For the binary features, like \"race:Asian\", \"race:AfricanAmerican\", \"race:Caucasian\", \"race:Hispanic\", \"hypertension\", \"heart_disease\" and \"diabetes\", 1 is always considered as an outlier. This is because the dataset is imbalanced for every one of this feature. <br>\n",
        "* The \"BMI\" feature contains a lot of outliers, as there are many people that have a BMI greater than 40 (morbid obesity), but also patients that have a BMI less than 15 (severe malnutrition).<br>\n",
        "* The features concerning the blood glucose level and hbA1c level also have 2-3 outliers, as some points are to be seen after 250 mg/dL for the blood glucose and after 8.5% for the hbA1c."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJT6Qq56Rsv3"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlsMy0yXA3I2"
      },
      "source": [
        "## &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**D. Duplicated values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOxZHKdZA9kh"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;Knowing if our dataset contains duplicated values is also useful and can be done through *data[data.duplicated()]* code. We can also use *duplicated.sum()* method to know the exact number of duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwQxgHXXtG_8"
      },
      "outputs": [],
      "source": [
        "print(f\"There are {data.duplicated().sum()} duplicated values that are:\")\n",
        "data[data.duplicated()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kitf5wEkBu57"
      },
      "source": [
        "Given the fact that we have 100 000 individuals in this dataset, removing 15 individuals will not have have a huge impact on the dataset. Therefore, we can simply delete those lines using *drop_duplicates()* function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr7G_Bm7t5hh"
      },
      "outputs": [],
      "source": [
        "data = data.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7UM985kxebG"
      },
      "source": [
        "To make sure that those lines were really deleted, let us use once again the *shape* method to display the number of lines (individuals) and columns (features)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JtK_neYxdzj"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRPWRVt8C648"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEQXC6WbC8hA"
      },
      "source": [
        "## &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**E. Visualization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL1X9Q5jZCAc"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;Let us now present an univariate analysis of different features from all the types (quantitative continuous, quantitative discrete, qualitative nominale, qualitative ordinal and binary).\n",
        "<br> In this part, we will work on:\n",
        "* bmi (quantitative continuous)\n",
        "* Age (quantitative discrete)\n",
        "* Location (qualitative nominale)\n",
        "* smoking_history (qualitative ordinal)\n",
        "* diabetes (binary)\n",
        "<br> &nbsp;&nbsp;&nbsp;&nbsp;We already know that every feature was measured for all the patients of the dataset, this is the reason why every feature's study will be done over the 99 985 patients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnd7odUafKRS"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpMkslSifL4M"
      },
      "source": [
        "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**1. Univariate analysis of bmi**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IKBBVj3fZZ6"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;Let us begin the univariate analysis of bmi, a quantitative continuous feature representing the ratio between the patient's weight and his height squared.\n",
        "<br>\n",
        "&nbsp;&nbsp;&nbsp;Those are the BMI categories for adults, according to the article written by the Center for Disease Control and Prevention:\n",
        "* **Underweight:** BMI $<$ 18.5\n",
        "* **Healthy Weight:** 18.5 $\\leq$ BMI $<$ 25.\n",
        "* **Overweight:** 25 $\\leq$ BMI $<$ 30.\n",
        "* **Obesity:** BMI $\\geq$ 30:<br>\n",
        "&nbsp;&nbsp;&nbsp; $-$   **First class obesity:** 30 $\\leq$ BMI $<$ 35 <br>\n",
        "&nbsp;&nbsp;&nbsp; $-$   **Second class obesity:** 35 $\\leq$ BMI $<$ 40. <br>\n",
        "&nbsp;&nbsp;&nbsp; $-$   **Third class (morbid obesity):** BMI $>$ 40. <br><br>\n",
        "&nbsp;&nbsp;&nbsp;Using the *describe()* method allows us to have the principal information about our feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7b31Q5GhbZ3"
      },
      "outputs": [],
      "source": [
        "data[\"bmi\"].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "binaPLDOCC0E"
      },
      "source": [
        "From what we see here, 50% of the patients have a BMI greater than 27 (overweight), while 25% have at most a healthy weight as the first quartile is 23.6.\n",
        "<br>Moreover, the minimum and maximum values show that there is a patient with a severe malnutrition (10.01), while one has an extreme obesity (95.69). <br>Finally, the fact that the standard deviation is equal to 6.6 shows that the distribution of BMI amongst the individuals from the dataset is very scattered, as we will see later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkOoB4oEB1Xr"
      },
      "source": [
        "<br>&nbsp;&nbsp;&nbsp;Because this feature is continuous, we can split the data into classes (5 bins for instance) and then display the histogram in value counts by classes of the feature. To better understand the proportion of each BMI class, we can also present the histogram by frequency and the cumulative distribution function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f35p7JvdJWz3"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (8, 6))\n",
        "valuecounts, bins, _ = plt.hist(data[\"bmi\"], bins = 5, edgecolor = \"black\", linewidth = 1.2) # We draw the histogram\n",
        "\n",
        "# We center the text indicating the class at the top of each rectangle of the histogram\n",
        "labels = [f\"[{round(bins[i], 1)} - {round(bins[i+1], 1)}[\" for i in range(len(bins) - 1)]\n",
        "centeredLabels = [(bins[i+1] + bins[i])/2 for i in range(len(bins) - 1)]\n",
        "for i, j, k in zip(centeredLabels, labels, valuecounts):\n",
        "  plt.text(i, k + max(valuecounts)/100, j, ha = \"center\", color = \"red\", fontweight = \"bold\")\n",
        "\n",
        "plt.title(\"Histogram by value counts of BMI classes\")\n",
        "plt.xlabel(\"BMI\")\n",
        "plt.ylabel(\"Occurrences of each class\")\n",
        "plt.grid(axis='y', linestyle='-', alpha=0.7) # We add a grid, to make the representation easier to understand\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_eB1Fq8FDxW"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (8, 6))\n",
        "valuecounts, bins, _ = plt.hist(data[\"bmi\"], bins = 5, edgecolor = \"black\", linewidth = 1.2, weights = np.ones(len(data)) / len(data)) # We add the parameter np.ones(len(data))/len(data) to print the histogram in frequency\n",
        "\n",
        "# We center the text indicating the class at the top of each rectangle of the histogram\n",
        "labels = [f\"[{round(bins[i], 1)} - {round(bins[i+1], 1)}[\" for i in range(len(bins) - 1)]\n",
        "centeredLabels = [(bins[i+1] + bins[i])/2 for i in range(len(bins) - 1)]\n",
        "for i, j, k in zip(centeredLabels, labels, valuecounts):\n",
        "  plt.text(i, k + max(valuecounts)/100, j, ha = \"center\", color = \"red\", fontweight = \"bold\")\n",
        "\n",
        "plt.title(\"Histogram by frequency of BMI classes\")\n",
        "plt.xlabel(\"BMI\")\n",
        "plt.ylabel(\"Occurrences of each class\")\n",
        "plt.grid(axis='y', linestyle='-', alpha=0.7) # We add a grid to make the representation easier to understand\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0u6KJukGLxI"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (8, 7))\n",
        "sns.ecdfplot(data = data, x = \"bmi\", linewidth = 1.5)\n",
        "plt.xlabel('BMI')\n",
        "plt.ylabel(\"Cumulative distribution Function of BMI\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKMpDpd1CdlS"
      },
      "source": [
        "These histograms indicate us that BMIs' values were separated into 5 classes:\n",
        "* $ [10, 27.1[$, which represents 40% of patients from the dataset ($\\approx$ 40 000 individuals) that are at most in overweight (underweight patients, healthy weight patients and overweight patients with a BMI between 25 and 27). <br>\n",
        "* $ [27.1, 44.3[$, which designates the 55 000 patients ($\\approx$ 55% of individuals) in overweight (with a BMI between 27 and 30) or in obesity (from class 1, 2 or 3) <br>\n",
        "* $ [44.3, 61.4[$, characterizing roughly 3 000 patients ($\\approx$ 3% of individuals) that have morbid obesity with a BMI between 44.3 and 61.4. <br>\n",
        "* $ [61.4, 78.6[$ and $[78.6, 95.7[$ that cumulate less than 1 000 patients ($\\approx$ 1% of patients) each and characterize the individuals with extreme obesity (BMI between 61.4 and 78.6 or 78.6 and 95.7).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlrVW0iDfaAt"
      },
      "source": [
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6AzmFf4fe4t"
      },
      "source": [
        "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**2. Univariate analysis of age**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4g94K50flOt"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;Let us now talk of the \"age\" feature, representing the age of the patient at the moment of the diabetes test. Once again, we begin with the *describe* method to display the principal information about the feature."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"age\"].describe()"
      ],
      "metadata": {
        "id": "Wf3RZmi1Zw4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;&nbsp;&nbsp;This array allows us to understand that the patient's age was approximately 42, while a patient was just a few months old, a few weeks old or a few days old as proves the minimum value of 0. The fact that the oldest patient was 80 years old justifies the high value of standard deviation (patients from all kinds of ages did the test).\n",
        "<br>&nbsp;&nbsp;&nbsp;Finally, 25% of the patients ($\\approx$ 24 996 patients) were at most 24 years old, while 50% of them were 43 years old and 24 996 patients were older than 60."
      ],
      "metadata": {
        "id": "BaIRHBg-bK4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>&nbsp;&nbsp;&nbsp;Before looking at the occurrencies and frequencies of this feature's values, let us simplify the study by splitting the values into 4 different classes: [0, 20], [21, 40], [41, 60] and [61, 80]."
      ],
      "metadata": {
        "id": "Tn0YjLfYeh0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = pd.cut(data[\"age\"], bins = [0, 20, 40, 60, 80], labels = [\"0-20\", \"21-40\", \"41-60\", \"61-80\"], include_lowest = True)\n",
        "pd.DataFrame({\"Occurrencies\": classes.value_counts().sort_index(), \"Frequencies\" : classes.value_counts(normalize = True).sort_index()})"
      ],
      "metadata": {
        "id": "2eYIUUDGetmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "L-FQMy88oa0K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK4DKgcQflOu"
      },
      "source": [
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFyvPOVZfnwt"
      },
      "source": [
        "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3. Univariate analysis of Location**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QagluUifnwv"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSR19aIqfnww"
      },
      "source": [
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYVplnUUfn5Z"
      },
      "source": [
        "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**4. Univariate analysis of smoking_history**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XivluL9fn5c"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvZuFsaSfn5c"
      },
      "source": [
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**5. Univariate analysis of diabetes**"
      ],
      "metadata": {
        "id": "IEzcOb6pXRVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JgFlrN2QXYYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "GVuLCMDqXWFi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyioGB7hxCPK"
      },
      "outputs": [],
      "source": [
        "print(data.describe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Sc8NHQ5ghhu"
      },
      "outputs": [],
      "source": [
        "# Visualize numeric distributions\n",
        "data.hist(figsize=(12, 8))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hkvyj5nzxe4"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y83_Ye3xzzH1"
      },
      "source": [
        "## &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**F. Data balance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upwkimsd08Gs"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;Let us now continue our analysis and preprocessing of the dataset by observing the balance of the feature \"diabetes\". This process will help us to know which models and pipelines will be used later.\n",
        "<br>&nbsp;&nbsp;&nbsp;First, let us use once again *value_counts()* method to print the number of occurrences of both classes (0 and 1) for the target feature: \"diabetes\".\n",
        "<br>&nbsp;&nbsp;&nbsp;We can also use *min* and *max* functions to calculate the ratio between the minority class and the majority one.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjzBxBHSqktF"
      },
      "outputs": [],
      "source": [
        "print(data[\"diabetes\"].value_counts())\n",
        "minority = data[\"diabetes\"].value_counts().min()\n",
        "majority = data[\"diabetes\"].value_counts().max()\n",
        "ratio = majority / minority\n",
        "print(f\"Imbalance ratio ≈ 1:{ratio:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgGZN6DD3amR"
      },
      "source": [
        "From what one can see here, the majoritarian class is the 0 (representing the patients without diabete), while the minority class is the 1 (representing the patients with diabete).<br>\n",
        "The fact that there are 10.76 times more patients without diabete than patients with diabete proves that this feature is highly imbalanced. To counter this imbalance, some models like RandomOverSampling or SMOTE will be used later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcZ7iuh94bEx"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OveNldAA4cUB"
      },
      "source": [
        "## &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**G. Correlation analysis and encoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wce6AIQ44s4"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;In this part, we will study the correlation between the different features and use encoding to split qualitative features into dummies ones (quantitative binary).\n",
        "<br>&nbsp;&nbsp;&nbsp;Let us first display the correlation matrix to see which variables are independent (correlation far from $\\pm1$) from the others and which ones directly depend from one another (correlation close to $\\pm1$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TKBVJANgvEy"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(11, 12)) # Creation of the figure that will represent the correlation matrix\n",
        "sns.heatmap(data.corr(numeric_only=True), annot=True, cmap='coolwarm') # We keep only quantitative variables and print the correlation matrix\n",
        "plt.title(\"Feature Correlation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7NfU1g3Nci8"
      },
      "source": [
        "If we look at the last line of this matrix, we can see that the features the most correlated to \"diabetes\" are:\n",
        "* blood_glucose_level (correlation of 0.42)\n",
        "* hbA1c_level (correlation of 0.4)\n",
        "* age (correlation of 0.26)\n",
        "However, the relationship between those variables and diabete is slight and qualitative features don't appear in the matrix.\n",
        "<br>To resolve this problem, let us transform the qualitative features into quantitative binary features using *pd.get_dummies* function. To make sure that the new features will take 0/1 values instead of \"True\"/\"False\", we can add the parameter: *dtype = int* at the end of the *get_dummies* function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdryeACag9iQ"
      },
      "outputs": [],
      "source": [
        "data_encoded = pd.get_dummies(data, columns = data.select_dtypes(include = \"object\").columns, drop_first=True, dtype = int)\n",
        "data_encoded.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGHHS84iQUph"
      },
      "source": [
        "Let us now see the new number of columns (features) and lines (individuals) using the *shape* function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZKMUnqUQdQO"
      },
      "outputs": [],
      "source": [
        "data_encoded.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctduoMLUQlzP"
      },
      "source": [
        "As one can see, approximately 60 new features appeared, and we now have 99 986 individuals and 74 features.\n",
        "<br>Because all the features are now quantitative, we can once again print the correlation matrix. To simplify the display, we will apply a filter: we will only keep the line of diabetes (because for the moment, only the relationships between the diabetes and the other features are important) and the features that have an absolute correlation of more than 0.25 with diabetes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQO_knNxR2Ca"
      },
      "outputs": [],
      "source": [
        "matrix = data_encoded.corr(numeric_only = True)\n",
        "matrix_temp = matrix[\"diabetes\"].drop(\"diabetes\") # We delete the cell showing the relationship between diabetes and itself (useless)\n",
        "matrix_filtered = matrix_temp[abs(matrix_temp) > 0.25] # We filter the matrix to keep the features having an absolute value correlation with diabetes greater than 0.25\n",
        "plt.figure(figsize = (3, 3))\n",
        "sns.heatmap(matrix_filtered.to_frame().T, annot = True, cmap = 'coolwarm') # We convert the matrix into a dataframe and print its transpose to have an horizontal result.\n",
        "plt.title(\"Correlation matrix keeping only features with an absolute correlation greater than 0.3\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fifjfA3mUzyT"
      },
      "source": [
        "From this final correlation matrix, we understand that the variables the most correlated to \"diabetes\" are the blood glucose level (correlation of 0.42), then the hbA1c_level (correlation of 0.4) and finally the age (0.26).\n",
        "<br>However, once again, those correlations with diabetes remain slight (for hba1c_level and blood_glucose_level) or even weak (for age)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIPi35iFgAl3"
      },
      "source": [
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnrF4Bj5hVUm"
      },
      "source": [
        "# **2. Train_test_split and implementation of algorithms and pipelines**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-NAj6wyhhpN"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;As we saw in the first part, the dataset's feature target is \"diabetes\", while the other features are explicative ones. This is the reason why the feature \"diabetes\" will be called *y* while the others will be grouped in a variable called *X*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFtleAWLiCnX"
      },
      "outputs": [],
      "source": [
        "X = data_encoded.drop('diabetes', axis=1)\n",
        "y = data_encoded['diabetes']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4neLDnoh0Jt"
      },
      "source": [
        "We can now use *train_test_split* function with a *test_size* of 0.3 to split our dataset into a training set and a test set. Because our dataset is moderately imbalanced (ratio $\\approx$ 11 as we saw before), we can add the parameter *stratify = y* to keep the same class proportions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZeMcWcgiEW4"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onFL-3a3YdO2"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Now that the separation between the training and the test has been done, we can develop some models to predict if a patient has diabete or not. Before beginning, let us define a function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlKKZSlciF5a"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary of models to compare\n",
        "models = {\n",
        "    # Logistic Regression: a linear model that estimates probabilities using a logistic function.\n",
        "    # max_iter=1000 ensures the solver has enough iterations to converge.\n",
        "    # random_state=42 ensures reproducibility of results.\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "\n",
        "    # Decision Tree: a non-linear model that splits data into branches based on feature values.\n",
        "    # random_state=42 ensures consistent tree structure across runs.\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "\n",
        "    # Random Forest: an ensemble of decision trees that improves accuracy and reduces overfitting.\n",
        "    # random_state=42 ensures reproducibility of the forest structure.\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVsmW9zhiNPb"
      },
      "outputs": [],
      "source": [
        "# Train baseline models\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"\\n{name} Performance:\")\n",
        "    from sklearn.metrics import classification_report\n",
        "    print(classification_report(y_test, model.predict(X_test)), \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygZ8Ziz9iPM-"
      },
      "source": [
        "# STEP 3: Handle Class Imbalance with SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BROniKc2iVJ0"
      },
      "outputs": [],
      "source": [
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "print(\"Resampled class distribution:\", Counter(y_resampled))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESNQsCAqiV56"
      },
      "outputs": [],
      "source": [
        "# Retrain Random Forest on resampled data\n",
        "rf_balanced = RandomForestClassifier(random_state=42)\n",
        "rf_balanced.fit(X_resampled, y_resampled)\n",
        "print(\"\\nRandom Forest (SMOTE) Performance:\")\n",
        "print(classification_report(y_test, rf_balanced.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ6yuwrcAoCd"
      },
      "source": [
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0TMlfY2-sa7"
      },
      "source": [
        "# **3. Data normalization and reduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQufrSzyAp8j"
      },
      "source": [
        "# **4. Analysis of the results and management of the overfitting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCLBjst7A9hF"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKSf5gHG92U7"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test, label=\"Model\"):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
        "    auc_roc = auc(fpr, tpr)\n",
        "    auc_pr = auc(recall, precision)\n",
        "\n",
        "    print(f\"\\n=== {label} ===\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(f\"AUC (ROC): {auc_roc:.3f} | AUC (PR): {auc_pr:.3f}\")\n",
        "\n",
        "    # Plot ROC\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=f\"{label} ROC (AUC={auc_roc:.2f})\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Precision-Recall\n",
        "    plt.figure()\n",
        "    plt.plot(recall, precision, label=f\"{label} PR (AUC={auc_pr:.2f})\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(\"Precision-Recall Curve\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate both models\n",
        "evaluate_model(models[\"Random Forest\"], X_test, y_test, label=\"Random Forest (Baseline)\")\n",
        "evaluate_model(rf_balanced, X_test, y_test, label=\"Random Forest (SMOTE)\")\n",
        "\n",
        "# Cost-Sensitive Evaluation\n",
        "def custom_cost(y_true, y_pred, fn_weight=10, fp_weight=1):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return fn_weight * fn + fp_weight * fp\n",
        "\n",
        "cost_baseline = custom_cost(y_test, models[\"Random Forest\"].predict(X_test))\n",
        "cost_smote = custom_cost(y_test, rf_balanced.predict(X_test))\n",
        "\n",
        "print(f\"Custom Cost (Baseline): {cost_baseline}\")\n",
        "print(f\"Custom Cost (SMOTE): {cost_smote}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3ZPx9ckyfpo"
      },
      "source": [
        "  RANDOM OVER SAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YMp8KLAvNJc"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('diabetes_dataset.csv')\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = df_encoded.drop('diabetes', axis=1)\n",
        "y = df_encoded['diabetes']\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIhzo2bTytD9"
      },
      "source": [
        "RANOM UNDER SAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHyPJzTkvTTj"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('diabetes_dataset.csv')\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = df_encoded.drop('diabetes', axis=1)\n",
        "y = df_encoded['diabetes']\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply RandomUnderSampler to balance the training data\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train a Random Forest model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bzjn5-0vN1B"
      },
      "source": [
        "HYPERPARAMETER TUNING FOR RANDOM FOREST USING RANDOM OVERSAMPLING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10kqN_K3ypHv"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcgssz_GRVds"
      },
      "outputs": [],
      "source": [
        "# Step 2: Load and preprocess the dataset\n",
        "data = pd.read_csv(\"diabetes_dataset.csv\")\n",
        "\n",
        "# One-hot encode categorical features\n",
        "data_encoded = pd.get_dummies(data, columns=['gender', 'location', 'smoking_history'], drop_first=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = data_encoded.drop('diabetes', axis=1)\n",
        "y = data_encoded['diabetes']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZZHud8GRVRp"
      },
      "outputs": [],
      "source": [
        "# Step 3: Split into training and test sets (stratified to preserve class balance)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "955DPB8JRU-V"
      },
      "outputs": [],
      "source": [
        "# Step 4: Apply Random OverSampling to balance the training data\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf4FO-55RUmR"
      },
      "outputs": [],
      "source": [
        "# Step 5: Scale the features (optional for Random Forest, but good practice)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83_YDcaOveT0"
      },
      "outputs": [],
      "source": [
        "# Step 6: Define the Random Forest model and hyperparameter grid\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv8SOVcovfTq"
      },
      "outputs": [],
      "source": [
        "# Step 7: Use RandomizedSearchCV for faster tuning\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=20,              # Try 20 random combinations\n",
        "    cv=2,                   # 3-fold cross-validation\n",
        "    scoring='f1',           # Optimize for F1-score\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF2ctR_Cve5k",
        "outputId": "b2330289-e96c-4bf3-df18-5589c6e0a71c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Fit the randomized search on the resampled training data\n",
        "random_search.fit(X_resampled_scaled, y_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4R7X4ivvz5h"
      },
      "outputs": [],
      "source": [
        "# Step 9: Retrieve the best model and parameters\n",
        "best_rf = random_search.best_estimator_\n",
        "print(\"Best Parameters:\", random_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6j0awOKivzmt"
      },
      "outputs": [],
      "source": [
        "# Step 10: Evaluate the tuned model on the original test set\n",
        "y_pred = best_rf.predict(X_test_scaled)\n",
        "y_proba = best_rf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"\\n=== Tuned Random Forest (Random OverSampling) ===\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDW0OodsvzR8"
      },
      "outputs": [],
      "source": [
        "# Step 11: Plot ROC and Precision-Recall curves\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(recall, precision, label=f\"PR Curve (AUC = {pr_auc:.2f})\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrNMw8blv_RL"
      },
      "source": [
        "LOGISTIC REGRESSION WITH RANDOM OVERSAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpUm3bmIyv5M"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4EeNewcv-2x"
      },
      "outputs": [],
      "source": [
        "# Step 2: Load and preprocess the dataset\n",
        "data = pd.read_csv(\"diabetes_dataset.csv\")\n",
        "\n",
        "# One-hot encode categorical features\n",
        "data_encoded = pd.get_dummies(data, columns=['gender', 'location', 'smoking_history'], drop_first=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = data_encoded.drop('diabetes', axis=1)\n",
        "y = data_encoded['diabetes']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0yDXPRWv-rR"
      },
      "outputs": [],
      "source": [
        "# Step 3: Split into training and test sets (stratified to preserve class balance)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7v1Z4pmv-fy"
      },
      "outputs": [],
      "source": [
        "# Step 4: Apply Random OverSampling to balance the training data\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GXxeuGAv-Rc"
      },
      "outputs": [],
      "source": [
        "# Step 5: Scale the features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFJJ5O42wTAF"
      },
      "outputs": [],
      "source": [
        "# Step 6: Define and train Logistic Regression with manually selected hyperparameters\n",
        "best_log_reg = LogisticRegression(\n",
        "    C=1.0,                    # Regularization strength (default = 1.0)\n",
        "    penalty='l2',             # L2 regularization (standard)\n",
        "    solver='liblinear',       # Works well with small datasets and L2\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "best_log_reg.fit(X_resampled_scaled, y_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xVV7AK7wSzX"
      },
      "outputs": [],
      "source": [
        "# Step 7: Evaluate the tuned model on the original test set\n",
        "y_pred = best_log_reg.predict(X_test_scaled)\n",
        "y_proba = best_log_reg.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"\\n=== Tuned Logistic Regression (Random OverSampling) ===\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfHkXbkZwSmK"
      },
      "outputs": [],
      "source": [
        "# Step 8: Plot ROC and Precision-Recall curves\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(recall, precision, label=f\"PR Curve (AUC = {pr_auc:.2f})\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ12-tajwbAK"
      },
      "source": [
        "DECISION TREE WITH OVERSAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJz4QXGMyz-r"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, precision_recall_curve, auc, accuracy_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iqXxKsOwbcX"
      },
      "outputs": [],
      "source": [
        "# Step 2: Load and preprocess the dataset\n",
        "data = pd.read_csv(\"diabetes_dataset.csv\")\n",
        "\n",
        "# One-hot encode categorical features\n",
        "data_encoded = pd.get_dummies(data, columns=['gender', 'location', 'smoking_history'], drop_first=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = data_encoded.drop('diabetes', axis=1)\n",
        "y = data_encoded['diabetes']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7Y1EHNwweNk"
      },
      "outputs": [],
      "source": [
        "# Step 3: Split into training and test sets (stratified to preserve class balance)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BHdAW6xwd93"
      },
      "outputs": [],
      "source": [
        "# Step 4: Apply Random OverSampling to balance the training data\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KCSDPewwduO"
      },
      "outputs": [],
      "source": [
        "# Step 5: Scale the features (optional for Decision Tree)\n",
        "scaler = StandardScaler()\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEblE0uSwdgo"
      },
      "outputs": [],
      "source": [
        "# Step 6: Define the Decision Tree model and hyperparameter grid\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [5, 10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-618B7qWwu5m"
      },
      "outputs": [],
      "source": [
        "# Step 7: Use RandomizedSearchCV for faster tuning\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=dt,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=20,              # Try 20 random combinations\n",
        "    cv=2,                   # 3-fold cross-validation\n",
        "    scoring='f1',           # Optimize for F1-score\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DO4pICXjww7l"
      },
      "outputs": [],
      "source": [
        "# Step 8: Fit the randomized search on the resampled training data\n",
        "random_search.fit(X_resampled_scaled, y_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74Xs9FybwzwC"
      },
      "outputs": [],
      "source": [
        "# Step 9: Retrieve the best model and parameters\n",
        "best_dt = random_search.best_estimator_\n",
        "print(\"Best Parameters:\", random_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGNYbL0Zwzjj"
      },
      "outputs": [],
      "source": [
        "# Step 10: Evaluate the tuned model on the original test set\n",
        "y_pred = best_dt.predict(X_test_scaled)\n",
        "y_proba = best_dt.predict_proba(X_test_scaled)[:, 1]\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\n=== Tuned Decision Tree (Random OverSampling) ===\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xh9B7ZBw51x"
      },
      "outputs": [],
      "source": [
        "# Step 11: Plot ROC and Precision-Recall curves\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(recall, precision, label=f\"PR Curve (AUC = {pr_auc:.2f})\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}